---
title: 大模型算法工程师专精知识清单
pubDatetime: 2024-09-26T04:00:00Z
author: author
tags:
  - 大语言模型
  - 算法工程师
  - 预训练模型
  - 模型微调
  - 模型部署
  - AI工程化
description: 面向10B+参数规模预训练模型的算法工程师知识清单，覆盖数据工程、模型训练、优化部署等完整链路
---

# 大模型专精知识清单（Markdown 速查版）

> 面向“已学完 DL 核心，想深耕 **10B+ 参数预训练/微调/部署**”的算法工程师  
> 按 **“数据→模型→训练→优化→对齐→部署→评测→安全”** 8 条链路组织，  
> 每条给出 **理论要点 + 代码/工具 + 学习资源 + 验证里程碑**。

---

## 1 数据工程（Data-Centric）

| 主题 | 必须掌握 | 工具/代码 | 验证里程碑 |
|---|---|---|---|
| 高质量去重 | MinHash LSH、URL 归一化、n-gram 去重 | `datasketch` + `pyspark` | 在 CommonCrawl 子集上把 **重复率降到 < 5 %** |
| 毒性/偏置过滤 | Detoxify、Perspective API、Llama-Guard | `transformers` 加载分类器 | PII 识别召回 **> 95 %**，毒性样本比例下降 **10×** |
| 数据配比 | cosine similarity 聚类→领域权重重采样 | `sentence-transformers` | 继续预训练 1 B 模型，**perplexity ↓ 8 %** |
| 词表/分词 | BPE vs BBPE vs Unigram、SentencePiece 算法 | `sentencepiece` 训练 | 30 GB 中文语料，**vocab 32k → 压缩率 0.73** |

资源：  
- Paper: *Data-centric AI for Large Language Models* (2024)  
- 课：Stanford CS329S *Data-centric AI*

---

## 2 模型架构（Modeling）

| 模块 | 理论要点 | 代码/实现 |
|---|---|---|
| 旋转位置编码 | RoPE 复数指数形式，远程衰减系数 θ | `transformers` 里 `apply_rotary_pos_emb` |
| GLU 变体 | SwiGLU、GeGLU 激活，参数量 ↑ 1.5× 但性能 ↑ | Llama-2 FFN `silu(gate) * up` |
| 并行化 | Tensor Parallel (TP)、Pipeline Parallel (PP)、Sequence Parallel | `torch.distributed.tensor.parallel` |
| 稀疏化 | MoE Top-K 路由、负载均衡 loss `aux_loss = α * load^2` | `fairseq-moe` / `Megatron-LM` |

验证项目：  
- 手撸 **1 B 参数 LLaMA 结构**（40 层，hidden 2048，intermediate 5504）  
  在 8×A100 40G 上 TP=2 PP=4，**显存占用 < 35 G/卡**

---

## 3 预训练（Training at Scale）

| 主题 | 关键公式/参数 | 工具 |
|---|---|---|
| 混合精度 | `torch.cuda.amp` + bfloat16，动态损失缩放 | Megatron-Core |
| 梯度累积 | `accumulate_steps = global_batch / (gpu_batch * gpu_num)` | DeepSpeed Stage-2 |
| 学习率调度 | WSD (Warmup-Stable-Decay) 阶段比例 1:8:1 | `transformers.get_linear_schedule_with_warmup` |
| 分布式日志 | TensorBoard + W&B，记录 **flops, tflops/gpu, loss_scale** | `wandb.log({"tflops": tflops})` |

里程碑：  
- 在 **1 B tokens** 上继续预训练 1 B 模型，**loss 从 2.80 → 2.45**  
- 训练效率 ≥ **180 TFLOPS/GPU**（A100 理论 312 的 58 %）

---

## 4 显存与吞吐优化（Efficiency）

| 技术 | 显存节省 | 代码入口 |
|---|---|---|
| ZeRO-3 | 4× | `deepspeed.zero.Init()` |
| FlashAttention-2 | 2–4× 序列长度线性 | `from flash_attn import flash_attn_func` |
| 激活重算 | 1.5× 换 30 % 计算 | `checkpoint_sequential` |
| CPU Offload | 无限 offload，速度 ↓ 20 % | `device_map="auto"` + `accelerate` |

验证：  
- 7 B 模型 + 4k seq_len + ZeRO-3 + FlashAttn，**单卡 80 G 可训 batch=1**  
- 推理：**vLLM + PagedAttention**，吞吐比 HF 提升 **24×**

---

## 5 对齐（Alignment）

| 算法 | 核心思想 | 实现 |
|---|---|---|
| RLHF | PPO 三模型（actor/ref/reward），KL 惩罚 β=0.1 | `trlx` / `openai/lm-human-preferences` |
| DPO | 直接偏好优化，无需 reward 模型 | `transformers.DPOTrainer` |
| RLAIF | 用 LLM 代替人类标注偏好 | Google RLAIF paper 代码 |
| 安全对齐 | Constitutional AI、红队对抗 | `llamaguard` 生成对抗 prompt |

里程碑：  
- 用 **50k 偏好对** 对 7 B 模型做 DPO，**Win-rate ↑ 18 %**（vs SFT）  
- 红队攻击成功率从 **28 % → 7 %**

---

## 6 部署与推理（Serving）

| 方案 | 延迟 (7B, 2048 in/128 out) | 工具 |
|---|---|---|
| HuggingFace | 1100 ms | `pipeline` |
| vLLM | 45 ms | `vllm --model llama-7b --tensor-parallel-size 2` |
| TensorRT-LLM | 38 ms | `trtllm-build` |
| 量化 INT8 | 显存 ↓ 2×，精度 ↓ < 1 % | `bitsandbytes` / `AWQ` |

任务：  
- 在 **2×A10G 24G** 上部署 **INT8 7 B 模型**，**首 token 延迟 < 100 ms**，**吞吐 1200 req/s**（continuous batching）

---

## 7 评测与基准（Evaluation）

| 维度 | 数据集 | 指标 |
|---|---|---|
| 知识 | MMLU、C-Eval | 5-shot accuracy |
| 推理 | GSM8K、MATH | pass@1 / maj@8 |
| 代码 | HumanEval、MBPP | pass@1 |
| 长文本 | “Needle in Haystack” | 检索准确率 vs 长度 |

自动化：  
```bash
pip install lm-eval-harness
lm_eval --model hf --model_args pretrained=llama-7b --tasks mmlu,gsm8k --batch_size 16
```

---

## 8 安全与合规（Safety & Governance）

- 指令攻击：Prompt Injection、越狱（DAN）、Token Smuggling  
- 隐私：训练数据 PII 遗忘（Unlearning）、差分隐私（DP-SGD ε=1）  
- 合规：中国《生成式 AI 管理办法》、EU AI Act、NIST AI RMF  
- 可解释：Logit Lens、Activation Patching、Mechanistic Interpretability

---

## 学习路径速览（按周）

| 周 | 主题 | 产出 |
|---|---|---|
| 1–2 | 数据工程 + 词表 | 30 GB 清洗脚本 + 32k SPM 模型 |
| 3–4 | 架构 + 并行 | 手撸 1 B LLaMA + 8 卡 TP/PP |
| 5–6 | 预训练 + 优化 | 继续预训练 loss ↓ 0.35 |
| 7 | 显存优化 | ZeRO-3 + FlashAttn 单卡可训 7 B |
| 8 | 对齐 | DPO 50k 偏好对，Win-rate ↑ 18 % |
| 9 | 部署 | vLLM INT8 部署，首 token < 100 ms |
| 10 | 评测 + 安全 | MMLU 5-shot 55 → 63，红队攻击 ↓ 21 % |

---

## 资源汇总（一键收藏）

- 书：  
  – *Designing Large-Scale Language Models*（2024 开源草稿）  
  – *Efficient Processing of Transformers*（2023）  
- 课程：  
  – Stanford CS25 *Transformers United*（YouTube 全集）  
  – Princeton COS597G *Advanced Topics in LLMs*  
- 代码库：  
  – [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)  
  – [vllm-project/vllm](https://github.com/vllm-project/vllm)  
  – [huggingface/trl](https://github.com/huggingface/trl)  

---

> 把以上 **8 条链路** 全部跑通一次，你就拥有 **“端到端大模型落地”** 的完整闭环，  
> 可胜任 **大模型预训练/微调/部署/对齐** 的全栈算法岗位。祝训练顺利，显存常够用！

# 数学理论
# 大模型方向需补充的数学基础（Markdown 速查）

> 已掌握“DL 通用数学”（高数/线代/概率/优化）后，**专精大模型**仍需补的 **增量数学点**  
> 按 **“够用→推导→发明”** 三级标注，方便按需投入时间。

---

## 1 随机过程 & 大数定律（Scaling Law 核心）

| 概念 | 大模型场景 | 推荐深度 |
|---|---|---|
| 次高斯/次指数随机变量 | 证明“训练损失→期望损失”的集中不等式 | 够用：知道 Hoeffding/Bernstein 形式 |
| 鞅差序列 & Azuma | 分析 Adam 误差界的论文随处可见 | 推导：会抄证明 |
| Scaling Law 纸面推导 | L(α, β, γ) = α·N^β + γ·D^δ 拟合 | 发明：能改形式（多模态、MoE） |

资源：  
- 书：*High-Dimensional Probability*（Roman Vershynin）第 1–2 章  
- 论文：*Scaling Laws for Neural Language Models* (OpenAI, 2020) 附录 B

---

## 2 矩阵分析（并行/低秩/量化）

| 工具 | 用途 | 掌握层级 |
|---|---|---|
| Singular Value Perturbation | 解释“大模型低秩微调”为何有效 | 推导：Weyl 不等式 + Davis-Kahan |
| Matrix Bernstein | 压缩通信时量化误差界 | 够用：会套定理 |
| Kronecker 积 & 向量化 | TP/PP 梯度合并的符号推导 | 推导：‖A⊗B‖ = ‖A‖·‖B‖ |
| 随机矩阵谱密度 | 研究“特征值长尾”与过度参数化 | 发明：Marchenko-Pastur 分布改缩放 |

代码验证：  
```python
import numpy as np
A = np.random.randn(2048, 2048) / np.sqrt(2048)
eigs = np.linalg.svd(A)[1]
print("最大奇异值≈2.0?", eigs[0])
```

---

## 3 信息论（损失函数、对齐、压缩）

| 概念 | 大模型应用 | 需掌握公式 |
|---|---|---|
| KL(q‖p) & 反向 KL | RLHF 的 KL 惩罚项 β·KL(πθ‖πref) | 会求导：∇θ KL = 𝔼[∇θ log π] |
| Mutual Information I(X;Z) | 衡量 prompt 与 hidden state 的“可解释位” | 够用：I ≤ H |
| Rate–Distortion-Parameter | 量化位数 vs 任务掉点理论下界 | 发明：改写 R(D, θ) |
| Minimal Description Length | 解释“参数量大但泛化好” | 推导：L = −log p(D|θ) + ½‖θ‖² |

资源：  
- 书：*Elements of Information Theory*（Cover）第 2、8、15 章  
- 论文：*RLHF from Scratch* 附录 KL 推导

---

## 4 随机优化与收敛理论（AdamW、LR Scheduler）

| 算法 | 需补的数学 | 推荐深度 |
|---|---|---|
| Adam 偏差修正 | β₁^t, β₂^t 的期望偏差项 | 推导：会还原 Kingma 原始论文附录 |
| LAMB / LARS 层级自适应 | ‖θ_l‖ / ‖g_l‖ 的层归一化 | 够用：知道层级学习率 |
| Warmup-Stable-Decay (WSD) | 分段常数 → 指数 decay 的收敛界 | 发明：能给新 scheduler 写 Lyapunov 证明 |
| 梯度噪声尺度 GNS | 确定 batch size 的“临界值” | 推导：Tr(G)/‖g‖² |

实验脚本：  
```bash
pip install transformers[torch] optimi
python -m optimi.gns wandb llama-7b 2048
```

---

## 5 测度论 & 泛函分析（可选，做理论/发明）

| 场景 | 工具 | 投入建议 |
|---|---|---|
| Transformer 是积分算子的离散化 | 核空间 L²(μ) → 注意力算子 | 纯理论方向再学 |
| Wasserstein 梯度流 | 解释“粒子视角”的 SGD | 同上 |
| Reproducing Kernel Hilbert Space | Neural Tangent Kernel 极限 | 同上 |

> 若目标 **工业落地**，可跳过；若投 **ICML/NeurIPS 理论 Track**，建议学：  
> 书：*A Course in Functional Analysis*（Conway）第 1–3 章

---

## 6 增量数学 → 代码对照表

| 数学结论 | 对应代码/日志 | 可见现象 |
|---|---|---|
| Matrix Bernstein 界 | 量化后 ‖W_q − W‖₂ ≤ ε | 下游任务掉点 < 1 % |
| Azuma 不等式 | Adam 梯度界 | loss 曲线 95 % 带内 |
| KL(πθ‖πref) ≤ β | `kl_div = (logprobs - ref_logprobs).mean()` | RLHF 不“崩” |
| Scaling Law 预测 | `python fit_scaling_law.py --N 7e9 --D 1.2e12` | 预测 loss 2.01 vs 实测 2.03 |

---

## 7 20-小时“快充”计划（边上班边学）

| 周次 | 内容 | 小时 | 产出 |
|---|---|---|---|
| 第 1 周 | 随机过程 Scaling Law 推导 | 5 | 复现 OpenAI 附录 B |
| 第 2 周 | 矩阵 Bernstein + 量化误差实验 | 5 | 写 notebook 给团队分享 |
| 第 3 周 | 信息论 KL & RLHF 推导 | 5 | 推导出 DPO 目标函数 |
| 第 4 周 | Adam 收敛界 + 实验验证 | 5 | 画出 GNS-Batch Size 曲线 |

---

## 结论

1. **工业落地**：补 1–4 够用 → 能读懂 paper、调超参、写证明级 blog；  
2. **研究/发明**：补 5 泛函 + 随机矩阵 → 发 ICML/NeurIPS 理论 Track；  
3. **验证标准**：随手给同事讲清“KL 惩罚为什么 β=0.1”且 5 分钟写完推导，即达标。祝你数学愉快，显存和公式都收敛！